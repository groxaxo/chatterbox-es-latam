# Chatterbox ES-LATAM

<div align="center">

**Sistema de S√≠ntesis de Voz (TTS) para Espa√±ol Latinoamericano**

*Servidor TTS avanzado con API compatible OpenAI e interfaz web moderna*

[![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)](https://python.org)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.100+-green.svg)](https://fastapi.tiangolo.com)
[![Docker](https://img.shields.io/badge/Docker-CUDA_12.1-blue.svg)](https://www.docker.com/)

</div>

---

## üéØ Caracter√≠sticas

- ‚ú® **TTS de Alta Calidad**: S√≠ntesis de voz natural optimizada para espa√±ol latinoamericano
- üéôÔ∏è **Clonaci√≥n de Voz**: Genera audio con voces personalizadas usando muestras de referencia  
- ‚ö° **Rendimiento GPU**: Soporte completo para CUDA (NVIDIA)
- üåê **API Compatible OpenAI**: Endpoint `/v1/audio/speech` compatible con OpenAI
- üé® **Interfaz Web**: UI intuitiva en espa√±ol con controles avanzados
- üìù **Textos Largos**: Procesamiento inteligente con chunking autom√°tico

---

## üöÄ Quick Start

### Docker (Recomendado)

```bash
# GPU
docker-compose up -d

# CPU only
docker build --build-arg RUNTIME=cpu -t chatterbox-es-latam .
docker run -p 8004:8004 chatterbox-es-latam
```

Abre `http://localhost:8004` en tu navegador.

### Instalaci√≥n Local

```bash
# 1. Clonar
git clone https://github.com/tu-usuario/chatterbox-es-latam.git
cd chatterbox-es-latam

# 2. Entorno virtual
python -m venv venv
source venv/bin/activate

# 3. Instalar dependencias
pip install -r requirements-nvidia.txt  # GPU
# o
pip install -r requirements.txt         # CPU

# 4. Iniciar
python server.py
```

---

## üéõÔ∏è Uso

### Interfaz Web

1. Abre `http://localhost:8004`
2. Escribe el texto a sintetizar
3. Selecciona una voz predefinida o sube audio de referencia
4. Ajusta par√°metros y haz clic en "Generar Audio"

### API REST

#### OpenAI-Compatible

```bash
curl -X POST http://localhost:8004/v1/audio/speech \
  -H "Content-Type: application/json" \
  -d '{
    "model": "chatterbox-es-latam",
    "input": "Hola, bienvenido al sistema TTS.",
    "voice": "default.wav",
    "response_format": "mp3",
    "speed": 1.0
  }' \
  --output audio.mp3
```

#### Custom Endpoint

```bash
curl -X POST http://localhost:8004/tts \
  -H "Content-Type: application/json" \
  -d '{
    "text": "Texto a sintetizar en espa√±ol latinoamericano.",
    "voice_mode": "predefined",
    "predefined_voice_id": "default.wav",
    "temperature": 0.8,
    "exaggeration": 1.0,
    "cfg_weight": 0.5,
    "speed_factor": 1.0,
    "output_format": "wav",
    "split_text": true,
    "chunk_size": 120,
    "language": "es"
  }' \
  --output audio.wav
```

---

## üìã API Reference

### Endpoints

| Endpoint | M√©todo | Descripci√≥n |
|----------|--------|-------------|
| `/health` | GET | Health check |
| `/v1/audio/speech` | POST | Generar audio (OpenAI-compatible) |
| `/tts` | POST | Generar audio (custom) |
| `/v1/audio/voices` | GET | Listar voces disponibles |
| `/v1/voices` | GET | Alias para `/v1/audio/voices` |
| `/v1/audio/models` | GET | Listar modelos disponibles |
| `/v1/models` | GET | Alias para `/v1/audio/models` |

### Par√°metros de Generaci√≥n

| Par√°metro | Rango | Default | Descripci√≥n |
|-----------|-------|---------|-------------|
| `temperature` | 0.0 - 1.5 | 0.8 | Aleatoriedad (menor = m√°s estable) |
| `exaggeration` | 0.25 - 2.0 | 1.0 | Expresividad de la voz |
| `cfg_weight` | 0.2 - 1.0 | 0.5 | Influencia en estilo |
| `speed_factor` | 0.25 - 4.0 | 1.0 | Velocidad del audio |
| `seed` | ‚â• 0 | 0 | Semilla para reproducibilidad |
| `split_text` | boolean | true | Dividir texto largo autom√°ticamente |
| `chunk_size` | 50 - 500 | 120 | Tama√±o de chunk para divisi√≥n de texto |
| `language` | string | "es" | Idioma del texto |

---

## üèóÔ∏è Arquitectura

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   CLIENTE       ‚îÇ         ‚îÇ   SERVIDOR (GPU NVIDIA)        ‚îÇ
‚îÇ   (Navegador)   ‚îÇ         ‚îÇ                                ‚îÇ
‚îÇ                 ‚îÇ  HTTP   ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  Env√≠a texto   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ  ‚îÇ  TTS Pipeline            ‚îÇ  ‚îÇ
‚îÇ                 ‚îÇ         ‚îÇ  ‚îÇ  1. Recibe texto         ‚îÇ  ‚îÇ
‚îÇ  Recibe audio  ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ  ‚îÇ  2. Carga modelo         ‚îÇ  ‚îÇ
‚îÇ                 ‚îÇ  WAV    ‚îÇ  ‚îÇ  3. Genera audio (GPU)   ‚îÇ  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ  ‚îÇ  4. Retorna audio        ‚îÇ  ‚îÇ
                            ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
                            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Stack**:
- FastAPI (Python)
- PyTorch + Chatterbox TTS
- CUDA para GPU

---

## üíª Requisitos

### Hardware

| Componente | M√≠nimo | Recomendado |
|------------|--------|-------------|
| GPU | RTX 3060 12GB | RTX 4090 / A100 |
| RAM | 16GB | 32GB |
| CPU | 4 cores | 8+ cores |
| Storage | 10GB | 50GB+ |

### Software

```
Docker 24.0+
NVIDIA Container Toolkit
CUDA 12.1+
Python 3.10+ (para desarrollo)
```

### Performance (RTX 4090)

| Texto | Latencia |
|-------|----------|
| Corto (~10 palabras) | ~200ms |
| Medio (~50 palabras) | ~800ms |
| Largo (~200 palabras) | ~3s |

---

## ‚ö° Benchmarks de Rendimiento

> Benchmarks medidos en producci√≥n. RTF = Real-Time Factor (tiempo de c√≥mputo / duraci√≥n del audio). RTF < 1 = m√°s r√°pido que en tiempo real.

### CPU (sin GPU) ‚Äî medido

| M√©trica | Valor |
|---------|-------|
| Carga del modelo | ~31s (desde cach√© HF) |
| RTF mediana | **5.0√ó** |
| RTF rango | 4.0√ó ‚Äì 7.0√ó |
| Texto corto (~3s audio) | ~15s |
| Texto medio (~8s audio) | ~40s |
| Texto largo (~15s audio) | ~75s |

*Medido a partir de 32 solicitudes en un servidor CPU (sin CUDA). La variaci√≥n se debe a la longitud del texto y la carga concurrente.*

### GPU CUDA ‚Äî medido / estimado

| Hardware | Carga modelo | RTF t√≠pico | Texto corto | Texto medio |
|----------|-------------|------------|-------------|-------------|
| RTX 3090 (medido) | ~13s | ~0.3√ó | ~1s | ~2.5s |
| RTX 4090 (estimado) | ~7s | ~0.1√ó | ~0.2s | ~0.8s |
| A100 (estimado) | ~7s | ~0.1√ó | ~0.2s | ~0.8s |

*Carga del modelo desde cach√© HuggingFace local. Primera solicitud incluye carga (lazy load).*

### Modos de Ahorro de VRAM

| Modo | VRAM activa | Latencia al despertar |
|------|------------|----------------------|
| Activo (GPU) | ~4‚Äì6 GB | 0s |
| Dormido (CPU offload) | **0 MB** | ~3‚Äì5s (mover pesos a GPU) |

El servidor entra en modo dormido autom√°ticamente tras **5 minutos** sin solicitudes (`idle_timeout_sec` en `config.yaml`).

### NF4 (bitsandbytes) ‚Äî an√°lisis real en este proyecto

| Escenario | VRAM asignada | RTF (menor es mejor) | Estado |
|-----------|----------------|----------------------|--------|
| FP16/BF16 (default) | ~2.99 GB | **~0.79 ‚Äì 0.87** | Recomendado |
| NF4 (678 capas cuantizadas) | ~0.60 GB | ~1.42 ‚Äì 1.97 | Solo si falta VRAM |

**Conclusi√≥n pr√°ctica**:
- ‚úÖ NF4 **s√≠ funciona** t√©cnicamente en este repo (cuantiza 678 capas con bitsandbytes).
- ‚úÖ Reduce fuertemente VRAM activa (~80% menos asignada).
- ‚ö†Ô∏è En nuestras pruebas, **empeora latencia/RTF** frente al modo FP16/BF16.
- ‚ö†Ô∏è Correlaci√≥n de transcripci√≥n con Whisper (DeepInfra) fue menor en NF4 que en FP16.

Por defecto, `gpu_optimizations.use_nf4_quantization` se mantiene en `false` para priorizar calidad/latencia.

---

## üìÅ Estructura

```
chatterbox-es-latam/
‚îú‚îÄ‚îÄ server.py              # Servidor FastAPI principal
‚îú‚îÄ‚îÄ engine.py              # Motor de inferencia
‚îú‚îÄ‚îÄ config.yaml            # Configuraci√≥n
‚îú‚îÄ‚îÄ requirements.txt       # Dependencias CPU
‚îú‚îÄ‚îÄ requirements-nvidia.txt # Dependencias GPU
‚îú‚îÄ‚îÄ docker-compose.yml     # Docker Compose
‚îú‚îÄ‚îÄ Dockerfile             # Docker build
‚îú‚îÄ‚îÄ voices/                # Voces predefinidas
‚îú‚îÄ‚îÄ reference_audio/       # Audios de referencia
‚îú‚îÄ‚îÄ outputs/               # Audios generados
‚îú‚îÄ‚îÄ logs/                  # Logs del servidor
‚îú‚îÄ‚îÄ ui/                    # Interfaz web
‚îú‚îÄ‚îÄ web/                   # React UI (desarrollo)
‚îî‚îÄ‚îÄ training/              # Scripts de fine-tuning
```

---

## üê≥ Deployment

### Opciones

| Opci√≥n | Uso | GPU | Costo |
|--------|-----|-----|-------|
| **Local/Docker** | Desarrollo | Opcional | Gratis |
| **RunPod** | Producci√≥n | ‚úÖ | ~$0.20/hr |
| **AWS/GCP** | Enterprise | ‚úÖ | Variable |

### Docker Compose

```yaml
services:
  chatterbox-tts:
    build: .
    ports:
      - "8004:8004"
    volumes:
      - ./voices:/app/voices
      - ./reference_audio:/app/reference_audio
      - ./outputs:/app/outputs
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
```

### RunPod

1. Crear Pod con template PyTorch + CUDA 12.1
2. GPU: RTX 3090 o superior
3. Clonar repo e instalar: `pip install -r requirements-nvidia.txt`
4. Ejecutar: `python server.py`

---

## üîí Seguridad

- HTTPS recomendado para producci√≥n
- Rate limiting en endpoints
- Validaci√≥n de inputs
- Audio temporal (no se almacena permanentemente)

---

## üôè Agradecimientos

- [Resemble AI](https://github.com/resemble-ai/chatterbox) por Chatterbox TTS

---

## üìù Licencia

MIT License
